{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "e20b0e4df7a48152c181e1c7af19584e1e85ca578c5a93b9a66dd3bb16fdafb3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data Mining of Solar Flares\n",
    "\n",
    "The Data provided is from the Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI, originally High Energy Solar Spectroscopic Imager or HESSI). It is a NASA solar flare observatory. <br>\n",
    "Description and analysis of the data is in the report, here is manipulation of data in various forms filtering,cleaning, and transforming the data as appropriate such that it can be used to produce optimal classification for the flare bassed on its energy oputput, in the form of MLP and RF classifiers.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import scikitplot as skplt\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import Model Classifiers.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import evaluation libaries.\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "source": [
    "# Data preperation\n",
    "### Data ingestion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting string to a datetime, specifying the format.\n",
    "# Datetime Formatting Codes: http://bit.ly/python-dt-fmt\n",
    "# Working with Dates and Time Series Data: https://youtu.be/UFuo7EHI8zc?t=641\n",
    "# Pandas can recognise the datetime format of this rhessi data, so it doesn't need this 'date_parser' method.\n",
    "# But doing this in case I need it for other data.\n",
    "d_parser = lambda x: datetime.strptime(x, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_rhessi_orig = pd.read_csv('../Data Sets/V1 Solar Flares from RHESSI Mission/hessi.solar.flare.2002to2016.csv'\n",
    ", parse_dates=['start.date'], date_parser=d_parser)\n",
    "# Read the original data in. \n",
    "df_rhessi = df_rhessi_orig.copy() # Create a copy to work with.\n",
    "print(type(df_rhessi)) # Check it's in the expected DataFrame format."
   ]
  },
  {
   "source": [
    "### Initial inspection of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rhessi.shape # Rows and Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rhessi.head(10) # Making sure the data has been injested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to parse to date time after reading in the data.\n",
    "#df_rhessi['start.date'] = pd.to_datetime(df_rhessi['start.date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0           Tuesday\n",
       "1           Tuesday\n",
       "2         Wednesday\n",
       "3         Wednesday\n",
       "4         Wednesday\n",
       "            ...    \n",
       "113937       Monday\n",
       "113938       Monday\n",
       "113939      Tuesday\n",
       "113940      Tuesday\n",
       "113941      Tuesday\n",
       "Name: start.date, Length: 113942, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "df_rhessi['start.date'].dt.day_name()"
   ]
  },
  {
   "source": [
    "\n",
    "### Look at the data types of each column/attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They do not match the briefe, this will need to be rectified later. \n",
    "# IPSI and Contra should be of type int.\n",
    "df_rhessi.dtypes"
   ]
  },
  {
   "source": [
    "### Inspect the Unique values of the categorical (dtype object), attributes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df_uniques and read in the columns unique values.\n",
    "df_uniques = pd.DataFrame(columns= ['energykev', 'flag1', 'flag2', 'flag3', 'flag4', 'flag5'])\n",
    "df_uniques.energykev = pd.Series(df_rhessi['energy.kev'].unique())\n",
    "df_uniques.flag1 = pd.Series(df_rhessi[\"flag.1\"].unique())\n",
    "df_uniques.flag2 = pd.Series(df_rhessi['flag.2'].unique())\n",
    "df_uniques.flag3 = pd.Series(df_rhessi['flag.3'].unique())\n",
    "df_uniques.flag4 = pd.Series(df_rhessi['flag.4'].unique())\n",
    "df_uniques.flag5 = pd.Series(df_rhessi['flag.5'].unique())\n",
    "df_uniques"
   ]
  },
  {
   "source": [
    "## Cleaning\n",
    "\n",
    "First we will look for nulls and drop them for now, later revisions we will do some imputing. <br>\n",
    "\n",
    "Fortunately the data from Kaggl is clean!! \n",
    "\n",
    "### Drop Null entries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rhessi.isnull().sum()) # Show nulls."
   ]
  },
  {
   "source": [
    "### Fixing the data types.\n",
    "\n",
    "I was able to parse the start.date column to type datetime as I read the .csv file in. <br>\n",
    "\n",
    "But for the time values, I would like them as just datetime.time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From our initial inspection, we know that some of our columns are of the wrong data type.\n",
    "# We want the date time to be of type date time.\n",
    "df_rhessi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e\n",
    "# I found that using a smaller int subtype means pandas requires less memory.\n",
    "# Consuming less memory is always better!\n",
    "df_rhessi['start.time'] = pd.to_datetime.time(df_rhessi['start.time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because I know the values for IPSI and Contra range from 0 to 101, it makes sense to use int8, only consuming 1 byte of memory.\n",
    "# To be safe I will cast to int and then downcast safely.\n",
    "df_cvd_cleaned.IPSI = df_cvd_cleaned.IPSI.astype(int)\n",
    "df_cvd_cleaned.IPSI = pd.to_numeric(df_cvd_cleaned.IPSI, downcast=('unsigned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error when casting Contra of type Object to int, there is an empty entry.\n",
    "# Find it here.  \n",
    "s_empty_contra = df_cvd_cleaned[df_cvd_cleaned.Contra == \" \"]\n",
    "s_empty_contra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty Contra here.\n",
    "df_cvd_cleaned = df_cvd_cleaned.drop(index=642)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Contra from object, to string, strip leading and trailing white spaces, cast to int, then downcast.\n",
    "df_cvd_cleaned.Contra = df_cvd_cleaned.Contra.astype(str).str.strip().astype(int)\n",
    "df_cvd_cleaned.Contra = pd.to_numeric(df_cvd_cleaned.Contra, downcast=('unsigned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert objects to nominal category, we don't care about order. Again this saves memory. \n",
    "df_cvd_cleaned.Indication = df_cvd_cleaned.Indication.astype('category')\n",
    "df_cvd_cleaned.Diabetes = df_cvd_cleaned.Diabetes.astype('category')\n",
    "df_cvd_cleaned.IHD = df_cvd_cleaned.IHD.astype('category')\n",
    "df_cvd_cleaned.Hypertension = df_cvd_cleaned.Hypertension.astype('category')\n",
    "df_cvd_cleaned.Arrhythmia = df_cvd_cleaned.Arrhythmia.astype('category')\n",
    "df_cvd_cleaned.History = df_cvd_cleaned.History.astype('category')\n",
    "df_cvd_cleaned.label = df_cvd_cleaned.label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now are data types are more representitve of the data.\n",
    "df_cvd_cleaned.dtypes"
   ]
  },
  {
   "source": [
    "## Data visualisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have created my own pallete to clearly convay risk in red and no risk in blue.\n",
    "NoRisk_Risk = [\"#2F70F2\", \"#D83A37\"]\n",
    "sns.set_palette(NoRisk_Risk)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "source": [
    "### Count plots, stacked with label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Count of Risk/NoRisk\")\n",
    "sns.countplot(data=df_cvd_cleaned, x='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Diabtese\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Diabetes']).size().reset_index().pivot(columns='label', index='Diabetes', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)\n",
    "#sns.countplot(data=df_cvd_cleaned, x='Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of IHD\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'IHD']).size().reset_index().pivot(columns='label', index='IHD', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title(\"Count of Hypertension\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Hypertension']).size().reset_index().pivot(columns='label', index='Hypertension', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Arrhythmia\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Arrhythmia']).size().reset_index().pivot(columns='label', index='Arrhythmia', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of History\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'History']).size().reset_index().pivot(columns='label', index='History', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Indication\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Indication']).size().reset_index().pivot(columns='label', index='Indication', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "source": [
    "### Looking at IPSI and Contra"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('IPSI vs Indication vs label')\n",
    "sns.boxplot(data=df_cvd_cleaned, x='Indication', y='IPSI', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Contra vs Indication vs label')\n",
    "sns.boxplot(data=df_cvd_cleaned, x='Indication', y='Contra', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=1.5)\n",
    "\n",
    "# Set up variables in a matrix formatt.\n",
    "df_ipsi_contra = df_cvd_cleaned.drop(columns=['Random', 'Id', 'Indication', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'label'])\n",
    "cvd_mx = df_ipsi_contra.corr() \n",
    "\n",
    "sns.heatmap(cvd_mx, annot=True, cmap='RdBu')\n",
    "# Slightly possitevely correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting IPSI against Contra, with the label being hue.\n",
    "sns.set_context('paper', font_scale=1.5)\n",
    "sns.jointplot(data=df_cvd_cleaned, x='IPSI', y='Contra', kind='scatter', hue='label')\n",
    "# Strong possitive correlation between Contra and IPSI leading to Risk."
   ]
  },
  {
   "source": [
    "# Encoding data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding Indication.\n",
    "onehot_indication = pd.get_dummies(df_cvd_cleaned.Indication, prefix='Indication')\n",
    "onehot_indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following can be dummy variables.\n",
    "dummy_diabetes = pd.get_dummies(df_cvd_cleaned.Diabetes, drop_first=True)\n",
    "dummy_diabetes.rename(columns={'yes' : 'Diabetes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_ihd = pd.get_dummies(df_cvd_cleaned.IHD, drop_first=True)\n",
    "dummy_ihd.rename(columns={'yes' : 'IHD'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hypertension = pd.get_dummies(df_cvd_cleaned.Hypertension, drop_first=True)\n",
    "dummy_hypertension.rename(columns={'yes' : 'Hypertension'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_arrhythmia = pd.get_dummies(df_cvd_cleaned.Arrhythmia, drop_first=True)\n",
    "dummy_arrhythmia.rename(columns={'yes' : 'Arrhythmia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_history = pd.get_dummies(df_cvd_cleaned.History, drop_first=True)\n",
    "dummy_history.rename(columns={'yes' : 'History'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_label = pd.get_dummies(df_cvd_cleaned.label, drop_first=True)"
   ]
  },
  {
   "source": [
    "## Feature Discretision - Chunking\n",
    "Feature Discretisation helps reduce the search space. <br>\n",
    "pd.cut allows us to create a new df and do some binning to the IPSI and Contra features. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels here as both features need to be chuncked the same to keep consistency.\n",
    "bins_to_chunk = [0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89,94,99,100]\n",
    "bin_labels = ['0', '5','10','15','20','25','30','35','40','45','50','55','60','65','70','75','80','85','90','95','100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ipsi = df_cvd_cleaned.IPSI\n",
    "bin_ipsi = pd.cut(bin_ipsi, bins=bins_to_chunk, labels=bin_labels) # Bin Ipsilateral.\n",
    "bin_ipsi.rename('IPSI_%', inplace=True) # Name the axis.\n",
    "bin_ipsi = pd.to_numeric(bin_ipsi, downcast=('unsigned')) # Downcast for efficieny.\n",
    "bin_ipsi.value_counts().sort_index() # Look at bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_ipsi.isnull().sum()) # Check we havn't lost any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contra already follows this patern, but bining will reduce search space.\n",
    "bin_contra = df_cvd_cleaned.Contra\n",
    "bin_contra = pd.cut(bin_contra, bins=bins_to_chunk, labels=bin_labels) # Bin contralaterol.\n",
    "bin_contra.rename('Contra_%', inplace=True) # Name the axis.\n",
    "bin_contra = pd.to_numeric(bin_contra, downcast=('unsigned')) # Downcast for efficieny.\n",
    "bin_contra.value_counts().sort_index() # Look at bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_contra.isnull().sum()) # Check we havn't lost any data."
   ]
  },
  {
   "source": [
    "## Amalagamate encoded features into a new df.\n",
    "While getting our data ready for training, I'm also going to drop the random, and id columns as they don't hold relevant information for the model. Multiple sessions could hold a patern, they could show how early symptoms do develop into a high risk of CVD over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old columns.\n",
    "df_cvd_encoded_set_0 = df_cvd_cleaned.drop(columns=['Random', 'Id', 'Indication', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'IPSI', 'Contra', 'label'])\n",
    "# concat new encoded columns.\n",
    "df_cvd_encoded_set_0 = pd.concat([df_cvd_encoded_set_0, onehot_indication, dummy_diabetes, dummy_ihd, dummy_hypertension, dummy_arrhythmia, dummy_history, bin_ipsi, bin_contra, dummy_label], axis=1)\n",
    "df_cvd_encoded_set_0"
   ]
  },
  {
   "source": [
    "### Now that the data is encoded, we can visualise some more patterns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Heatmap of all features')\n",
    "sns.set_context('paper', font_scale=1.1)\n",
    "sns.heatmap(data=df_cvd_encoded_set_0.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_cvd_encoded_set_0, x='Arrhythmia', y='Contra_%', hue='Risk')\n",
    "# Identified possible outlier, there is a no risk data point that has high contra and arrhythmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable to get selection to work, Contra_% is acting strange.\n",
    "# df_cvd_encoded_set_0[(df_cvd_encoded_set_0.Arrhythmia == 1) & (df_cvd_encoded_set_0.Contra_% > 70)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_cvd_encoded_set_0, x='Arrhythmia', y='IPSI_%', hue='Risk')"
   ]
  },
  {
   "source": [
    "## Feature Selection\n",
    "I am creating 3 new data sets based on learning what features can be dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvd_encoded_set_1 = df_cvd_encoded_set_0.drop(columns=['History'])\n",
    "df_cvd_encoded_set_1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off Set 1 that already has history dropped.\n",
    "df_cvd_encoded_set_2 = df_cvd_encoded_set_1.drop(columns=['Indication_A-F', 'Indication_CVA'])\n",
    "df_cvd_encoded_set_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off Set 1 that already has history dropped.\n",
    "# Set 3 added after Random Forrest feature importance.\n",
    "df_cvd_encoded_set_3 = df_cvd_encoded_set_1.drop(columns=['Indication_ASX'])\n",
    "df_cvd_encoded_set_3.head(1)"
   ]
  },
  {
   "source": [
    "## Sanity checks\n",
    "\n",
    "First I would like to manually look at a few entries and compare the raw data to the encoded data to ensure that the encoding has been done correctly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked because of IPSI and Contra, wanted to ensure binning was correct. It initially wasn't.\n",
    "df_cvd_encoded_set_0.loc[[922]] # A-F,no,no,yes,no,no,75.0,50 ,NoRisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvd_encoded_set_0.dtypes # Sanity check, ensure data is all numerical and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cvd_encoded_set_0.isnull().sum()) # Sanity check, ensure there are no nulls."
   ]
  },
  {
   "source": [
    "# Modelling\n",
    "## Split the data\n",
    "\n",
    "Now our data is ready, we want to create our training and testing sets. <br>\n",
    "\n",
    "Our truth, target value y, will be Risk, as that's what we want our model to predict. <br>\n",
    "\n",
    "Our training data, inputs x, will be everything other than Risk. <br>\n",
    "\n",
    "We split our data 70 / 30. We train on 70% of the data and then test on 30%.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data sets here.\n",
    "df_model_data = df_cvd_encoded_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target set Y - Risk.\n",
    "y = df_model_data.Risk\n",
    "y # is series, 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set x - Everything BUT Risk.\n",
    "x = df_model_data.drop(columns=['Risk'])\n",
    "x # is data frame, 2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "source": [
    "## Multi Layer Perceptron (MLP) classifier.\n",
    "\n",
    "Now that we have our data sets split, we can pass it to our model for training. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='relu', solver='sgd', learning_rate='adaptive') # MLP 1\n",
    "# model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='identity', solver='lbfgs') # MLP 2,3,4,5\n",
    "model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='tanh', solver='adam', batch_size=500, beta_1=0.8, beta_2=0.75) # MLP 6 \n",
    "\n",
    "model.fit(x_train,y_train) # Training - Fit our data to the model.\n",
    "\n",
    "pred_y = model.predict(x_test) # Predict. \n",
    "\n",
    "accuracy_score(y_test, pred_y, normalize=True)"
   ]
  },
  {
   "source": [
    "## Evaluation metrics - Confusion matrix and F2 score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used our model to predict (x_test) above, now we are comparing that with the truth (y_test).\n",
    "sns_plot = skplt.metrics.plot_confusion_matrix(y_test, pred_y, normalize=True) \n",
    "sns_plot.figure.savefig(\"Second model.png\") # Save it as we will go through and change some things."
   ]
  },
  {
   "source": [
    "# Random Forest Classifier\n",
    "https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to label the feature importance.\n",
    "# Need to make df with labels for each of the 4 different data sets otherwise they don't match up.\n",
    "df_cvd_feature_names_set_0 = ['Indication A-F', 'Indication ASX', 'Indication CVA', 'Indication TIA', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'IPSI', 'Contra']\n",
    "\n",
    "df_cvd_feature_names_set_3 = ['Indication A-F', 'Indication CVA', 'Indication TIA', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'IPSI', 'Contra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, criterion='gini')\n",
    "rf.fit( x_train, y_train ) # Use the same split data as above.\n",
    "y_pred_test = rf.predict(x_test) # Predict using random forrest.\n",
    "rf.score( x_test, y_test ) # View accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test)) # View the classification report for test data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the feature importance, labeled.\n",
    "# Only working for data set 0 atm.\n",
    "for name, score in zip( df_cvd_feature_names_set_3, rf.feature_importances_ ):\n",
    "    print(name, score)\n",
    "# From this I can see that ASX and History have low importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = skplt.metrics.plot_confusion_matrix(y_test, y_pred_test, normalize=True) "
   ]
  }
 ]
}