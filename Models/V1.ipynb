{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "e20b0e4df7a48152c181e1c7af19584e1e85ca578c5a93b9a66dd3bb16fdafb3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data Mining of Solar Flares\n",
    "\n",
    "The Data provided is from the Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI, originally High Energy Solar Spectroscopic Imager or HESSI). It is a NASA solar flare observatory. <br>\n",
    "Description and analysis of the data is in the report, here is manipulation of data in various forms filtering,cleaning, and transforming the data as appropriate such that it can be used to produce optimal classification for the flare bassed on its energy oputput, in the form of MLP and RF classifiers.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import scikitplot as skplt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import Model Classifiers.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import evaluation libaries.\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "source": [
    "# Data preperation\n",
    "### Data ingestion.\n",
    "\n",
    "How to convert txt to csv: https://stackoverflow.com/questions/39642082/convert-txt-to-csv-python-script <br>\n",
    "\n",
    "How to remove excess white space: https://stackoverflow.com/questions/2077897/substitute-multiple-whitespace-with-single-whitespace-in-python"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting txt to csv.\n",
    "with open('../Data Sets/V1 Solar Flares from RHESSI Mission/hessi_2018.txt', 'r') as in_file:\n",
    "    stripped = (' '.join(line.split()) for line in in_file)    \n",
    "    lines = (line.split(\" \") for line in stripped if line)\n",
    "    with open('../Data Sets/V1 Solar Flares from RHESSI Mission/hessi_2018.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(('Flare', 'Start_Date', 'Start_Time', 'Peak_Time', 'End_Time', 'Dur_S', 'Peak_c/s', 'Total_Counts', 'Energy_keV', 'X_Pos_Arcsec', 'Y_Pos_Arcsec', 'Radial_Pos_Arcsec', 'Active_Region', 'F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11'))\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting string to a datetime, specifying the format.\n",
    "# Datetime Formatting Codes: http://bit.ly/python-dt-fmt\n",
    "# Working with Dates and Time Series Data: https://youtu.be/UFuo7EHI8zc?t=641\n",
    "# Pandas can recognise the datetime format of this rhessi data, so it doesn't need this 'date_parser' method.\n",
    "# But doing this in case I need it for other data.\n",
    "d_parser = lambda x: datetime.strptime(x, '%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kaylu\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_rhessi_orig = pd.read_csv('../Data Sets/V1 Solar Flares from RHESSI Mission/hessi_2018.csv'\n",
    ", parse_dates=['Start_Date'], date_parser=d_parser)\n",
    "# Read the original data in. \n",
    "df_rhessi = df_rhessi_orig.copy() # Create a copy to work with.\n",
    "print(type(df_rhessi)) # Check it's in the expected DataFrame format."
   ]
  },
  {
   "source": [
    "### Initial inspection of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(121206, 24)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df_rhessi.shape # Rows and Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Flare Start_Date Start_Time Peak_Time  End_Time  Dur_S  Peak_c/s  \\\n",
       "0  2021213 2002-02-12   21:29:56  21:33:38  21:41:48    712       136   \n",
       "1  2021228 2002-02-12   21:44:08  21:45:06  21:48:56    288         7   \n",
       "2  2021332 2002-02-13   00:53:24  00:54:54  00:57:00    216        15   \n",
       "3  2021308 2002-02-13   04:22:52  04:23:50  04:26:56    244        20   \n",
       "4  2021310 2002-02-13   07:03:52  07:05:14  07:07:48    236       336   \n",
       "5  2021353 2002-02-13   07:07:48  07:09:14  07:20:56    788       272   \n",
       "6  2021354 2002-02-13   07:20:56  07:22:42  07:30:04    548        28   \n",
       "7  2021312 2002-02-13   08:53:20  08:55:18  09:05:08    708        92   \n",
       "8  2021339 2002-02-13   10:02:56  10:04:42  10:04:44    108        26   \n",
       "9  2021313 2002-02-13   12:29:32  12:30:58  12:33:24    232        26   \n",
       "\n",
       "   Total_Counts Energy_keV  X_Pos_Arcsec  ...  F02  F03  F04  F05  F06  F07  \\\n",
       "0        167304      12-25           592  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "1          9504       6-12           604  ...   P1   PE   Q1  NaN  NaN  NaN   \n",
       "2         11448       6-12          -310  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "3         17400      12-25          -277  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "4        313392      25-50          -272  ...   GS   P1   PE   Q2  NaN  NaN   \n",
       "5        524304      12-25          -271  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "6         52488       6-12          -267  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "7        125352      25-50          -362  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "8         10368       6-12          -235  ...   P1   PE   Q2   SE  NaN  NaN   \n",
       "9         16920      12-25          -905  ...   P1  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   F08  F09  F10  F11  \n",
       "0  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  \n",
       "5  NaN  NaN  NaN  NaN  \n",
       "6  NaN  NaN  NaN  NaN  \n",
       "7  NaN  NaN  NaN  NaN  \n",
       "8  NaN  NaN  NaN  NaN  \n",
       "9  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[10 rows x 24 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Flare</th>\n      <th>Start_Date</th>\n      <th>Start_Time</th>\n      <th>Peak_Time</th>\n      <th>End_Time</th>\n      <th>Dur_S</th>\n      <th>Peak_c/s</th>\n      <th>Total_Counts</th>\n      <th>Energy_keV</th>\n      <th>X_Pos_Arcsec</th>\n      <th>...</th>\n      <th>F02</th>\n      <th>F03</th>\n      <th>F04</th>\n      <th>F05</th>\n      <th>F06</th>\n      <th>F07</th>\n      <th>F08</th>\n      <th>F09</th>\n      <th>F10</th>\n      <th>F11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021213</td>\n      <td>2002-02-12</td>\n      <td>21:29:56</td>\n      <td>21:33:38</td>\n      <td>21:41:48</td>\n      <td>712</td>\n      <td>136</td>\n      <td>167304</td>\n      <td>12-25</td>\n      <td>592</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021228</td>\n      <td>2002-02-12</td>\n      <td>21:44:08</td>\n      <td>21:45:06</td>\n      <td>21:48:56</td>\n      <td>288</td>\n      <td>7</td>\n      <td>9504</td>\n      <td>6-12</td>\n      <td>604</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>PE</td>\n      <td>Q1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021332</td>\n      <td>2002-02-13</td>\n      <td>00:53:24</td>\n      <td>00:54:54</td>\n      <td>00:57:00</td>\n      <td>216</td>\n      <td>15</td>\n      <td>11448</td>\n      <td>6-12</td>\n      <td>-310</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021308</td>\n      <td>2002-02-13</td>\n      <td>04:22:52</td>\n      <td>04:23:50</td>\n      <td>04:26:56</td>\n      <td>244</td>\n      <td>20</td>\n      <td>17400</td>\n      <td>12-25</td>\n      <td>-277</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021310</td>\n      <td>2002-02-13</td>\n      <td>07:03:52</td>\n      <td>07:05:14</td>\n      <td>07:07:48</td>\n      <td>236</td>\n      <td>336</td>\n      <td>313392</td>\n      <td>25-50</td>\n      <td>-272</td>\n      <td>...</td>\n      <td>GS</td>\n      <td>P1</td>\n      <td>PE</td>\n      <td>Q2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2021353</td>\n      <td>2002-02-13</td>\n      <td>07:07:48</td>\n      <td>07:09:14</td>\n      <td>07:20:56</td>\n      <td>788</td>\n      <td>272</td>\n      <td>524304</td>\n      <td>12-25</td>\n      <td>-271</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2021354</td>\n      <td>2002-02-13</td>\n      <td>07:20:56</td>\n      <td>07:22:42</td>\n      <td>07:30:04</td>\n      <td>548</td>\n      <td>28</td>\n      <td>52488</td>\n      <td>6-12</td>\n      <td>-267</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2021312</td>\n      <td>2002-02-13</td>\n      <td>08:53:20</td>\n      <td>08:55:18</td>\n      <td>09:05:08</td>\n      <td>708</td>\n      <td>92</td>\n      <td>125352</td>\n      <td>25-50</td>\n      <td>-362</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2021339</td>\n      <td>2002-02-13</td>\n      <td>10:02:56</td>\n      <td>10:04:42</td>\n      <td>10:04:44</td>\n      <td>108</td>\n      <td>26</td>\n      <td>10368</td>\n      <td>6-12</td>\n      <td>-235</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>PE</td>\n      <td>Q2</td>\n      <td>SE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2021313</td>\n      <td>2002-02-13</td>\n      <td>12:29:32</td>\n      <td>12:30:58</td>\n      <td>12:33:24</td>\n      <td>232</td>\n      <td>26</td>\n      <td>16920</td>\n      <td>12-25</td>\n      <td>-905</td>\n      <td>...</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df_rhessi.head(10) # Making sure the data has been injested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0           Tuesday\n",
       "1           Tuesday\n",
       "2         Wednesday\n",
       "3         Wednesday\n",
       "4         Wednesday\n",
       "            ...    \n",
       "121201       Friday\n",
       "121202       Friday\n",
       "121203     Saturday\n",
       "121204       Monday\n",
       "121205     Saturday\n",
       "Name: Start_Date, Length: 121206, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "df_rhessi['Start_Date'].dt.day_name() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timestamp('2002-02-12 00:00:00')"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df_rhessi['Start_Date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timestamp('2018-03-03 00:00:00')"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "df_rhessi['Start_Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timedelta('5863 days 00:00:00')"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# How many days worth of data do we have?\n",
    "df_rhessi['Start_Date'].max() - df_rhessi['Start_Date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (df_rhessi['start.date'] >= '')"
   ]
  },
  {
   "source": [
    "\n",
    "### Look at the data types of each column/attribute."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Flare                         int64\n",
       "Start_Date           datetime64[ns]\n",
       "Start_Time                   object\n",
       "Peak_Time                    object\n",
       "End_Time                     object\n",
       "Dur_S                         int64\n",
       "Peak_c/s                      int64\n",
       "Total_Counts                  int64\n",
       "Energy_keV                   object\n",
       "X_Pos_Arcsec                  int64\n",
       "Y_Pos_Arcsec                  int64\n",
       "Radial_Pos_Arcsec             int64\n",
       "Active_Region                 int64\n",
       "F01                          object\n",
       "F02                          object\n",
       "F03                          object\n",
       "F04                          object\n",
       "F05                          object\n",
       "F06                          object\n",
       "F07                          object\n",
       "F08                          object\n",
       "F09                          object\n",
       "F10                          object\n",
       "F11                          object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# They do not match the briefe, this will need to be rectified later. \n",
    "# IPSI and Contra should be of type int.\n",
    "df_rhessi.dtypes"
   ]
  },
  {
   "source": [
    "### Inspect the Unique values of the categorical (dtype object), attributes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Buffer  Energy_keV  F01  F02  F03  F04  F05  F06  F07  F08  F09  F10  F11\n",
       "0        1       12-25   A1   P1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "1        2        6-12   A0   GS   PE   Q1   Q2   PE   Q1   Q4   Q5   SS   SE\n",
       "2        3       25-50   a0   GE   P1   PE   SE   Q3   SE   Q3   SE   Q6   SD\n",
       "3        4         3-6   a1   PS   PS   Q2   Q1   Q2   Q3   SE   SD   Q5  NaN\n",
       "4        5      50-100   A3   A1   GE   PS   P1   SE   PE   PE   PE   SD  NaN\n",
       "5        6     100-300  NaN   a1   a2   a3   PS   Q4   Q4   SD   Q4   SE  NaN\n",
       "6        7     300-800  NaN   GD   GS   P1   PE   P1   Q2   Q7   Q6  NaN  NaN\n",
       "7        8    800-7000  NaN   ES   Q1   SE   Q3   PS   SD   Q5   PS  NaN  NaN\n",
       "8        9  7000-20000  NaN   EE   ES   GE   SS   SS   PS   P1   SS  NaN  NaN\n",
       "9       10         NaN  NaN   PE   EE   GS   GE   Q1   SS   PS  NaN  NaN  NaN\n",
       "10      11         NaN  NaN   DF   GD   EE   GD   SD   Q5   Q6  NaN  NaN  NaN\n",
       "11      12         NaN  NaN   A3   DF   ES   ES   GE   P1   SS  NaN  NaN  NaN\n",
       "12      13         NaN  NaN   a3   A3   DF   GS   ES   Q6  NaN  NaN  NaN  NaN\n",
       "13      14         NaN  NaN   DR   a3   SD   EE   EE   GD  NaN  NaN  NaN  NaN\n",
       "14      15         NaN  NaN   NS   Q2   GD   SD   GS   GE  NaN  NaN  NaN  NaN\n",
       "15      16         NaN  NaN   a2   Q3   Q3   Q4   GD   GS  NaN  NaN  NaN  NaN\n",
       "16      17         NaN  NaN  NaN   DR   Q4   DR   Q5   ES  NaN  NaN  NaN  NaN\n",
       "17      18         NaN  NaN  NaN   NS   DR   NS   NS  NaN  NaN  NaN  NaN  NaN\n",
       "18      19         NaN  NaN  NaN  NaN   NS  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "19      20         NaN  NaN  NaN  NaN   SS  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "20      21         NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "21      22         NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "22      23         NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "23      24         NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Buffer</th>\n      <th>Energy_keV</th>\n      <th>F01</th>\n      <th>F02</th>\n      <th>F03</th>\n      <th>F04</th>\n      <th>F05</th>\n      <th>F06</th>\n      <th>F07</th>\n      <th>F08</th>\n      <th>F09</th>\n      <th>F10</th>\n      <th>F11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>12-25</td>\n      <td>A1</td>\n      <td>P1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>6-12</td>\n      <td>A0</td>\n      <td>GS</td>\n      <td>PE</td>\n      <td>Q1</td>\n      <td>Q2</td>\n      <td>PE</td>\n      <td>Q1</td>\n      <td>Q4</td>\n      <td>Q5</td>\n      <td>SS</td>\n      <td>SE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>25-50</td>\n      <td>a0</td>\n      <td>GE</td>\n      <td>P1</td>\n      <td>PE</td>\n      <td>SE</td>\n      <td>Q3</td>\n      <td>SE</td>\n      <td>Q3</td>\n      <td>SE</td>\n      <td>Q6</td>\n      <td>SD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>3-6</td>\n      <td>a1</td>\n      <td>PS</td>\n      <td>PS</td>\n      <td>Q2</td>\n      <td>Q1</td>\n      <td>Q2</td>\n      <td>Q3</td>\n      <td>SE</td>\n      <td>SD</td>\n      <td>Q5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>50-100</td>\n      <td>A3</td>\n      <td>A1</td>\n      <td>GE</td>\n      <td>PS</td>\n      <td>P1</td>\n      <td>SE</td>\n      <td>PE</td>\n      <td>PE</td>\n      <td>PE</td>\n      <td>SD</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>100-300</td>\n      <td>NaN</td>\n      <td>a1</td>\n      <td>a2</td>\n      <td>a3</td>\n      <td>PS</td>\n      <td>Q4</td>\n      <td>Q4</td>\n      <td>SD</td>\n      <td>Q4</td>\n      <td>SE</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>300-800</td>\n      <td>NaN</td>\n      <td>GD</td>\n      <td>GS</td>\n      <td>P1</td>\n      <td>PE</td>\n      <td>P1</td>\n      <td>Q2</td>\n      <td>Q7</td>\n      <td>Q6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>800-7000</td>\n      <td>NaN</td>\n      <td>ES</td>\n      <td>Q1</td>\n      <td>SE</td>\n      <td>Q3</td>\n      <td>PS</td>\n      <td>SD</td>\n      <td>Q5</td>\n      <td>PS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>7000-20000</td>\n      <td>NaN</td>\n      <td>EE</td>\n      <td>ES</td>\n      <td>GE</td>\n      <td>SS</td>\n      <td>SS</td>\n      <td>PS</td>\n      <td>P1</td>\n      <td>SS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>PE</td>\n      <td>EE</td>\n      <td>GS</td>\n      <td>GE</td>\n      <td>Q1</td>\n      <td>SS</td>\n      <td>PS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>DF</td>\n      <td>GD</td>\n      <td>EE</td>\n      <td>GD</td>\n      <td>SD</td>\n      <td>Q5</td>\n      <td>Q6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>A3</td>\n      <td>DF</td>\n      <td>ES</td>\n      <td>ES</td>\n      <td>GE</td>\n      <td>P1</td>\n      <td>SS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a3</td>\n      <td>A3</td>\n      <td>DF</td>\n      <td>GS</td>\n      <td>ES</td>\n      <td>Q6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>DR</td>\n      <td>a3</td>\n      <td>SD</td>\n      <td>EE</td>\n      <td>EE</td>\n      <td>GD</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NS</td>\n      <td>Q2</td>\n      <td>GD</td>\n      <td>SD</td>\n      <td>GS</td>\n      <td>GE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>a2</td>\n      <td>Q3</td>\n      <td>Q3</td>\n      <td>Q4</td>\n      <td>GD</td>\n      <td>GS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>DR</td>\n      <td>Q4</td>\n      <td>DR</td>\n      <td>Q5</td>\n      <td>ES</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NS</td>\n      <td>DR</td>\n      <td>NS</td>\n      <td>NS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# Create a new df_uniques and read in the columns unique values.\n",
    "df_uniques = pd.DataFrame(columns= ['Buffer', 'Energy_keV', 'F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11'])\n",
    "\n",
    "df_uniques.Buffer = pd.Series(range(1,25))\n",
    "# When creating a df, the number of rows is set by the first column.\n",
    "# I needed to add this buffer to show all values.\n",
    "df_uniques.Energy_keV = pd.Series(df_rhessi.Energy_keV.unique())\n",
    "df_uniques.F01 = pd.Series(df_rhessi.F01.unique())\n",
    "df_uniques.F02 = pd.Series(df_rhessi.F02.unique())\n",
    "df_uniques.F03 = pd.Series(df_rhessi.F03.unique())\n",
    "df_uniques.F04 = pd.Series(df_rhessi.F04.unique())\n",
    "df_uniques.F05 = pd.Series(df_rhessi.F05.unique())\n",
    "df_uniques.F06 = pd.Series(df_rhessi.F06.unique())\n",
    "df_uniques.F07 = pd.Series(df_rhessi.F07.unique())\n",
    "df_uniques.F08 = pd.Series(df_rhessi.F08.unique())\n",
    "df_uniques.F09 = pd.Series(df_rhessi.F09.unique())\n",
    "df_uniques.F10 = pd.Series(df_rhessi.F10.unique())\n",
    "df_uniques.F11 = pd.Series(df_rhessi.F11.unique())\n",
    "\n",
    "df_uniques"
   ]
  },
  {
   "source": [
    "## Cleaning\n",
    "\n",
    "First we will look for nulls and drop them for now, later revisions we will do some imputing. <br>\n",
    "\n",
    "Fortunately the data from Kaggl is clean!! \n",
    "\n",
    "### Drop Null entries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flare                     0\nStart_Date                0\nStart_Time                0\nPeak_Time                 0\nEnd_Time                  0\nDur_S                     0\nPeak_c/s                  0\nTotal_Counts              0\nEnergy_keV                0\nX_Pos_Arcsec              0\nY_Pos_Arcsec              0\nRadial_Pos_Arcsec         0\nActive_Region             0\nF01                       0\nF02                       0\nF03                   19925\nF04                   20220\nF05                   62832\nF06                  101142\nF07                  117178\nF08                  120306\nF09                  121078\nF10                  121184\nF11                  121204\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_rhessi.isnull().sum()) # Show nulls."
   ]
  },
  {
   "source": [
    "### Fixing the data types.\n",
    "\n",
    "I was able to parse the start.date column to type datetime as I read the .csv file in. <br>\n",
    "\n",
    "But for the time values, I would like them as just datetime.time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Flare                         int64\n",
       "Start_Date           datetime64[ns]\n",
       "Start_Time                   object\n",
       "Peak_Time                    object\n",
       "End_Time                     object\n",
       "Dur_S                         int64\n",
       "Peak_c/s                      int64\n",
       "Total_Counts                  int64\n",
       "Energy_keV                   object\n",
       "X_Pos_Arcsec                  int64\n",
       "Y_Pos_Arcsec                  int64\n",
       "Radial_Pos_Arcsec             int64\n",
       "Active_Region                 int64\n",
       "F01                          object\n",
       "F02                          object\n",
       "F03                          object\n",
       "F04                          object\n",
       "F05                          object\n",
       "F06                          object\n",
       "F07                          object\n",
       "F08                          object\n",
       "F09                          object\n",
       "F10                          object\n",
       "F11                          object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "# From our initial inspection, we know that some of our columns are of the wrong data type.\n",
    "# We want the date time to be of type date time.\n",
    "df_rhessi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@vincentteyssier/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e\n",
    "# I found that using a smaller int subtype means pandas requires less memory.\n",
    "# Consuming less memory is always better!\n",
    "df_rhessi['start.time'] = pd.to_datetime.time(df_rhessi['start.time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because I know the values for IPSI and Contra range from 0 to 101, it makes sense to use int8, only consuming 1 byte of memory.\n",
    "# To be safe I will cast to int and then downcast safely.\n",
    "df_cvd_cleaned.IPSI = df_cvd_cleaned.IPSI.astype(int)\n",
    "df_cvd_cleaned.IPSI = pd.to_numeric(df_cvd_cleaned.IPSI, downcast=('unsigned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error when casting Contra of type Object to int, there is an empty entry.\n",
    "# Find it here.  \n",
    "s_empty_contra = df_cvd_cleaned[df_cvd_cleaned.Contra == \" \"]\n",
    "s_empty_contra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty Contra here.\n",
    "df_cvd_cleaned = df_cvd_cleaned.drop(index=642)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Contra from object, to string, strip leading and trailing white spaces, cast to int, then downcast.\n",
    "df_cvd_cleaned.Contra = df_cvd_cleaned.Contra.astype(str).str.strip().astype(int)\n",
    "df_cvd_cleaned.Contra = pd.to_numeric(df_cvd_cleaned.Contra, downcast=('unsigned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert objects to nominal category, we don't care about order. Again this saves memory. \n",
    "df_cvd_cleaned.Indication = df_cvd_cleaned.Indication.astype('category')\n",
    "df_cvd_cleaned.Diabetes = df_cvd_cleaned.Diabetes.astype('category')\n",
    "df_cvd_cleaned.IHD = df_cvd_cleaned.IHD.astype('category')\n",
    "df_cvd_cleaned.Hypertension = df_cvd_cleaned.Hypertension.astype('category')\n",
    "df_cvd_cleaned.Arrhythmia = df_cvd_cleaned.Arrhythmia.astype('category')\n",
    "df_cvd_cleaned.History = df_cvd_cleaned.History.astype('category')\n",
    "df_cvd_cleaned.label = df_cvd_cleaned.label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now are data types are more representitve of the data.\n",
    "df_cvd_cleaned.dtypes"
   ]
  },
  {
   "source": [
    "## Data visualisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have created my own pallete to clearly convay risk in red and no risk in blue.\n",
    "NoRisk_Risk = [\"#2F70F2\", \"#D83A37\"]\n",
    "sns.set_palette(NoRisk_Risk)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "source": [
    "### Count plots, stacked with label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Count of Risk/NoRisk\")\n",
    "sns.countplot(data=df_cvd_cleaned, x='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Diabtese\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Diabetes']).size().reset_index().pivot(columns='label', index='Diabetes', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)\n",
    "#sns.countplot(data=df_cvd_cleaned, x='Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of IHD\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'IHD']).size().reset_index().pivot(columns='label', index='IHD', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title(\"Count of Hypertension\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Hypertension']).size().reset_index().pivot(columns='label', index='Hypertension', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Arrhythmia\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Arrhythmia']).size().reset_index().pivot(columns='label', index='Arrhythmia', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of History\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'History']).size().reset_index().pivot(columns='label', index='History', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Count of Indication\")\n",
    "df_plot = df_cvd_cleaned.groupby(['label', 'Indication']).size().reset_index().pivot(columns='label', index='Indication', values=0)\n",
    "df_plot.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "source": [
    "### Looking at IPSI and Contra"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('IPSI vs Indication vs label')\n",
    "sns.boxplot(data=df_cvd_cleaned, x='Indication', y='IPSI', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Contra vs Indication vs label')\n",
    "sns.boxplot(data=df_cvd_cleaned, x='Indication', y='Contra', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=1.5)\n",
    "\n",
    "# Set up variables in a matrix formatt.\n",
    "df_ipsi_contra = df_cvd_cleaned.drop(columns=['Random', 'Id', 'Indication', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'label'])\n",
    "cvd_mx = df_ipsi_contra.corr() \n",
    "\n",
    "sns.heatmap(cvd_mx, annot=True, cmap='RdBu')\n",
    "# Slightly possitevely correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting IPSI against Contra, with the label being hue.\n",
    "sns.set_context('paper', font_scale=1.5)\n",
    "sns.jointplot(data=df_cvd_cleaned, x='IPSI', y='Contra', kind='scatter', hue='label')\n",
    "# Strong possitive correlation between Contra and IPSI leading to Risk."
   ]
  },
  {
   "source": [
    "# Encoding data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding Indication.\n",
    "onehot_indication = pd.get_dummies(df_cvd_cleaned.Indication, prefix='Indication')\n",
    "onehot_indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following can be dummy variables.\n",
    "dummy_diabetes = pd.get_dummies(df_cvd_cleaned.Diabetes, drop_first=True)\n",
    "dummy_diabetes.rename(columns={'yes' : 'Diabetes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_ihd = pd.get_dummies(df_cvd_cleaned.IHD, drop_first=True)\n",
    "dummy_ihd.rename(columns={'yes' : 'IHD'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_hypertension = pd.get_dummies(df_cvd_cleaned.Hypertension, drop_first=True)\n",
    "dummy_hypertension.rename(columns={'yes' : 'Hypertension'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_arrhythmia = pd.get_dummies(df_cvd_cleaned.Arrhythmia, drop_first=True)\n",
    "dummy_arrhythmia.rename(columns={'yes' : 'Arrhythmia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_history = pd.get_dummies(df_cvd_cleaned.History, drop_first=True)\n",
    "dummy_history.rename(columns={'yes' : 'History'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_label = pd.get_dummies(df_cvd_cleaned.label, drop_first=True)"
   ]
  },
  {
   "source": [
    "## Feature Discretision - Chunking\n",
    "Feature Discretisation helps reduce the search space. <br>\n",
    "pd.cut allows us to create a new df and do some binning to the IPSI and Contra features. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels here as both features need to be chuncked the same to keep consistency.\n",
    "bins_to_chunk = [0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89,94,99,100]\n",
    "bin_labels = ['0', '5','10','15','20','25','30','35','40','45','50','55','60','65','70','75','80','85','90','95','100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ipsi = df_cvd_cleaned.IPSI\n",
    "bin_ipsi = pd.cut(bin_ipsi, bins=bins_to_chunk, labels=bin_labels) # Bin Ipsilateral.\n",
    "bin_ipsi.rename('IPSI_%', inplace=True) # Name the axis.\n",
    "bin_ipsi = pd.to_numeric(bin_ipsi, downcast=('unsigned')) # Downcast for efficieny.\n",
    "bin_ipsi.value_counts().sort_index() # Look at bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_ipsi.isnull().sum()) # Check we havn't lost any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contra already follows this patern, but bining will reduce search space.\n",
    "bin_contra = df_cvd_cleaned.Contra\n",
    "bin_contra = pd.cut(bin_contra, bins=bins_to_chunk, labels=bin_labels) # Bin contralaterol.\n",
    "bin_contra.rename('Contra_%', inplace=True) # Name the axis.\n",
    "bin_contra = pd.to_numeric(bin_contra, downcast=('unsigned')) # Downcast for efficieny.\n",
    "bin_contra.value_counts().sort_index() # Look at bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_contra.isnull().sum()) # Check we havn't lost any data."
   ]
  },
  {
   "source": [
    "## Amalagamate encoded features into a new df.\n",
    "While getting our data ready for training, I'm also going to drop the random, and id columns as they don't hold relevant information for the model. Multiple sessions could hold a patern, they could show how early symptoms do develop into a high risk of CVD over time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old columns.\n",
    "df_cvd_encoded_set_0 = df_cvd_cleaned.drop(columns=['Random', 'Id', 'Indication', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'IPSI', 'Contra', 'label'])\n",
    "# concat new encoded columns.\n",
    "df_cvd_encoded_set_0 = pd.concat([df_cvd_encoded_set_0, onehot_indication, dummy_diabetes, dummy_ihd, dummy_hypertension, dummy_arrhythmia, dummy_history, bin_ipsi, bin_contra, dummy_label], axis=1)\n",
    "df_cvd_encoded_set_0"
   ]
  },
  {
   "source": [
    "### Now that the data is encoded, we can visualise some more patterns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Heatmap of all features')\n",
    "sns.set_context('paper', font_scale=1.1)\n",
    "sns.heatmap(data=df_cvd_encoded_set_0.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_cvd_encoded_set_0, x='Arrhythmia', y='Contra_%', hue='Risk')\n",
    "# Identified possible outlier, there is a no risk data point that has high contra and arrhythmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable to get selection to work, Contra_% is acting strange.\n",
    "# df_cvd_encoded_set_0[(df_cvd_encoded_set_0.Arrhythmia == 1) & (df_cvd_encoded_set_0.Contra_% > 70)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_cvd_encoded_set_0, x='Arrhythmia', y='IPSI_%', hue='Risk')"
   ]
  },
  {
   "source": [
    "## Feature Selection\n",
    "I am creating 3 new data sets based on learning what features can be dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvd_encoded_set_1 = df_cvd_encoded_set_0.drop(columns=['History'])\n",
    "df_cvd_encoded_set_1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off Set 1 that already has history dropped.\n",
    "df_cvd_encoded_set_2 = df_cvd_encoded_set_1.drop(columns=['Indication_A-F', 'Indication_CVA'])\n",
    "df_cvd_encoded_set_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off Set 1 that already has history dropped.\n",
    "# Set 3 added after Random Forrest feature importance.\n",
    "df_cvd_encoded_set_3 = df_cvd_encoded_set_1.drop(columns=['Indication_ASX'])\n",
    "df_cvd_encoded_set_3.head(1)"
   ]
  },
  {
   "source": [
    "## Sanity checks\n",
    "\n",
    "First I would like to manually look at a few entries and compare the raw data to the encoded data to ensure that the encoding has been done correctly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked because of IPSI and Contra, wanted to ensure binning was correct. It initially wasn't.\n",
    "df_cvd_encoded_set_0.loc[[922]] # A-F,no,no,yes,no,no,75.0,50 ,NoRisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvd_encoded_set_0.dtypes # Sanity check, ensure data is all numerical and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cvd_encoded_set_0.isnull().sum()) # Sanity check, ensure there are no nulls."
   ]
  },
  {
   "source": [
    "# Modelling\n",
    "## Split the data\n",
    "\n",
    "Now our data is ready, we want to create our training and testing sets. <br>\n",
    "\n",
    "Our truth, target value y, will be Risk, as that's what we want our model to predict. <br>\n",
    "\n",
    "Our training data, inputs x, will be everything other than Risk. <br>\n",
    "\n",
    "We split our data 70 / 30. We train on 70% of the data and then test on 30%.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data sets here.\n",
    "df_model_data = df_cvd_encoded_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target set Y - Risk.\n",
    "y = df_model_data.Risk\n",
    "y # is series, 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set x - Everything BUT Risk.\n",
    "x = df_model_data.drop(columns=['Risk'])\n",
    "x # is data frame, 2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "source": [
    "## Multi Layer Perceptron (MLP) classifier.\n",
    "\n",
    "Now that we have our data sets split, we can pass it to our model for training. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP model.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "# model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='relu', solver='sgd', learning_rate='adaptive') # MLP 1\n",
    "# model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='identity', solver='lbfgs') # MLP 2,3,4,5\n",
    "model = MLPClassifier(hidden_layer_sizes=(11,14,2), max_iter=1000, activation='tanh', solver='adam', batch_size=500, beta_1=0.8, beta_2=0.75) # MLP 6 \n",
    "\n",
    "model.fit(x_train,y_train) # Training - Fit our data to the model.\n",
    "\n",
    "pred_y = model.predict(x_test) # Predict. \n",
    "\n",
    "accuracy_score(y_test, pred_y, normalize=True)"
   ]
  },
  {
   "source": [
    "## Evaluation metrics - Confusion matrix and F2 score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used our model to predict (x_test) above, now we are comparing that with the truth (y_test).\n",
    "sns_plot = skplt.metrics.plot_confusion_matrix(y_test, pred_y, normalize=True) \n",
    "sns_plot.figure.savefig(\"Second model.png\") # Save it as we will go through and change some things."
   ]
  },
  {
   "source": [
    "# Random Forest Classifier\n",
    "https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to label the feature importance.\n",
    "# Need to make df with labels for each of the 4 different data sets otherwise they don't match up.\n",
    "df_cvd_feature_names_set_0 = ['Indication A-F', 'Indication ASX', 'Indication CVA', 'Indication TIA', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'History', 'IPSI', 'Contra']\n",
    "\n",
    "df_cvd_feature_names_set_3 = ['Indication A-F', 'Indication CVA', 'Indication TIA', 'Diabetes', 'IHD', 'Hypertension', 'Arrhythmia', 'IPSI', 'Contra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, criterion='gini')\n",
    "rf.fit( x_train, y_train ) # Use the same split data as above.\n",
    "y_pred_test = rf.predict(x_test) # Predict using random forrest.\n",
    "rf.score( x_test, y_test ) # View accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test)) # View the classification report for test data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the feature importance, labeled.\n",
    "# Only working for data set 0 atm.\n",
    "for name, score in zip( df_cvd_feature_names_set_3, rf.feature_importances_ ):\n",
    "    print(name, score)\n",
    "# From this I can see that ASX and History have low importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = skplt.metrics.plot_confusion_matrix(y_test, y_pred_test, normalize=True) "
   ]
  }
 ]
}